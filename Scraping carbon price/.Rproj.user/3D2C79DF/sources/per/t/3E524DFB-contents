```{r}
library(reticulate)
virtualenv_create("my-python")
use_virtualenv("my-python", required = TRUE)
virtualenv_install(envname = "my-python", "pandas", ignore_installed = FALSE, pip_options = character())
virtualenv_install(envname = "my-python", "bs4", ignore_installed = FALSE, pip_options = character())
virtualenv_install(envname = "my-python", "selenium", ignore_installed = FALSE, pip_options = character())
virtualenv_install(envname = "my-python", "html5lib", ignore_installed = FALSE, pip_options = character())
virtualenv_install(envname = "my-python", "lxml", ignore_installed = FALSE, pip_options = character()) 
virtualenv_install(envname = "my-python", "openpyxl", ignore_installed = FALSE, pip_options = character()) 
```

```{python}
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
import lxml
```

# 天津
```{python}
driver = webdriver.Firefox()
driver.get("https://www.chinatcx.com.cn/list/13.html?page=1")
```

```{python}
soup = BeautifulSoup(driver.page_source, 'html.parser')
lnks = driver.find_elements(By.TAG_NAME, 'a')
all_links = []
for lnk in lnks:
   all_links.append(lnk.get_attribute("href"))
```

```{python}
# Get all tables

all_tables = []
for i in range(41):
    driver.get("https://www.chinatcx.com.cn/list/13.html?page={}".format(i+1)) 
    driver.implicitly_wait(10)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    tables = soup.find_all('table')
    dfs = pd.read_html(str(tables), flavor='html5lib')
    all_tables.append(dfs)
```

```{python}
# Get dataframe
df_tables = pd.DataFrame()
for j in range(41):
    df_tables = pd.concat([df_tables, all_tables[j][0]], axis=0).reset_index(drop=True)
df_tables = pd.concat([df_tables, all_tables[41][0]], axis=0).reset_index(drop=True)
```

```{python}
# Output 
df_tables.to_excel("TJ_20230130_20131226_Carbon.xlsx") 
```

# 全国

```{python}
driver = webdriver.Firefox()
driver.get("https://www.chinatcx.com.cn/list/123.html")
```

```{python}
# Get all tables

  all_tables = []
  for i in range(19):
      driver.get("https://www.chinatcx.com.cn/list/123.html?page={}".format(i+1)) 
      driver.implicitly_wait(10)
      soup = BeautifulSoup(driver.page_source, 'html.parser')
      tables = soup.find_all('table')
      dfs = pd.read_html(str(tables), flavor='html5lib')
      all_tables.append(dfs)
```

```{python}
# Get dataframe
df_tables = pd.DataFrame()
for j in range(19):
    df_tables = pd.concat([df_tables, all_tables[j][0]], axis=0).reset_index(drop=True)
```

```{python}
# Output 
df_tables.to_excel("ALL_20230131_20210716_Carbon.xlsx") 
```

# 广州

# 深圳

```{python}
driver = webdriver.Firefox()
driver.get("https://szets.com/dailynewsCN/index.htm")
```

```{python}
# Get all tables
all_tables = []
for i in range(476):
    driver.get("https://szets.com/dailynewsCN/index_{}.htm".format(i+1)) 
    driver.implicitly_wait(10)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    tables = soup.find_all('table')
    dfs = pd.read_html(str(tables), flavor='html5lib')
    all_tables.append(dfs)
```

```{python}
# Get dataframe
df_tables = pd.DataFrame()
for j in range(476):
    df_tables = pd.concat([df_tables, all_tables[j][0]], axis=0).reset_index(drop=True)
```

```{python}
# Output 
df_tables.to_excel("SZ_20230131_20150518_Carbon.xlsx") 
```

# 重庆

```{python}
driver = webdriver.Firefox()
driver.get("https://tpf.cqggzy.com/itf/themes/cqkfts/index.html")
```

```{python}
# Get all tables
all_tables = []
for i in range(476):
    driver.get("https://szets.com/dailynewsCN/index_{}.htm".format(i+1)) 
    driver.implicitly_wait(10)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    tables = soup.find_all('table')
    dfs = pd.read_html(str(tables), flavor='html5lib')
    all_tables.append(dfs)
```

```{python}
# Get dataframe
df_tables = pd.DataFrame()
for j in range(1):
    df_tables = df_tables.merge([df_tables, all_tables[0][j]], how = 'inner')

all_tables[0][1].columns = all_tables[0][0].columns
```

```{python}
# Output 
all_tables[0][1].to_excel("CQ_20230131_20140619_Carbon.xlsx") 
```

# 四川

```{python}
driver = webdriver.Firefox()
driver.get("https://ets.sceex.com.cn/history.htm?k=li_shi_xing_qing&url=mrhq_ls&pageIndex=1")
```

```{python}
# Get all tables
all_tables = []
for i in range(98):
    driver.get("https://ets.sceex.com.cn/history.htm?k=li_shi_xing_qing&url=mrhq_ls&pageIndex={}".format(i+1)) 
    driver.implicitly_wait(10)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    tables = soup.find_all('table')
    dfs = pd.read_html(str(tables), flavor='html5lib')
    all_tables.append(dfs)
```

```{python}
# Get dataframe
df_tables = pd.DataFrame()
for j in range(98):
    df_tables = pd.concat([df_tables, all_tables[j][0]], axis=0).reset_index(drop=True)

```

```{python}
# Output 
df_tables.to_excel("SC_20230131_20161216_Carbon.xlsx") 
```



# 上海

```{python}
driver = webdriver.Firefox()
driver.get("https://www.cneeex.com/cneeex/daytrade/detail?SiteID=122#;")
```

```{python}
# Get all tables
all_tables = []
for i in range(98):
    driver.get("https://ets.sceex.com.cn/history.htm?k=li_shi_xing_qing&url=mrhq_ls&pageIndex={}".format(i+1)) 
    driver.implicitly_wait(10)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    tables = soup.find_all('table')
    dfs = pd.read_html(str(tables), flavor='html5lib')
    all_tables.append(dfs)
```


```{r}

```








```{r}
library("readxl")
library(tidyverse)
```

```{r}
HB <- read_excel("/Users/tonygong/Downloads/HB.xlsx")
colnames(HB) = c('Date', 'Value')
HB$Value = gsub('元','',HB$Value)
HB$Value = gsub('吨','',HB$Value)
HB$Value = gsub('成交量','',HB$Value)
HB$Value = gsub('成交价','',HB$Value)
HB$Value = gsub('\r\n湖北 : ','',HB$Value)
HB$Value = substring(HB$Value, 11)

new_HB = str_split_fixed(HB$Value, ':', 2) %>% as.tibble()
new_HB['Date']= HB$Date

new_HB = new_HB[,c(3,1,2)]
colnames(new_HB) = c("Date", "Price", "Volume")

```

```{r}
library(writexl)
write_xlsx(new_HB,"/Users/tonygong/Downloads/new_HB.xlsx")
```

